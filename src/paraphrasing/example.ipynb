{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART (Bidirectional and Auto-Regressive Transformer)\n",
    "\n",
    "https://huggingface.co/facebook/bart-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496f481589ee428cbe54161f3996086a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebd6cfcaea34c3f916823fac1bc4b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e72f255823415089a7e5ec32a6702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ada7b5a24b848ecb05d9a20db2cfb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e869fbf3ff0745038c6910bbb5f5c76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: She was a storm, not the kind you run from, but the kind you chase.\n",
      "Paraphrase: She was a storm, not the kind you run from, but the kind that you chase.\n",
      "\n",
      "Original: In the end, we only regret the chances we didn't take.\n",
      "Paraphrase: In the end, we only regret the chances we didn't take.\n",
      "\n",
      "Original: She wasn't looking for a knight, she was looking for a sword.\n",
      "Paraphrase: She wasn't looking at a knight, she was looking for a sword.\n",
      "\n",
      "Original: I dreamt I am running on sand in the night\n",
      "Paraphrase: I dreamt I am running on sand in the night\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Set up input sentences\n",
    "sentences = [\n",
    "    \"She was a storm, not the kind you run from, but the kind you chase.\",\n",
    "    \"In the end, we only regret the chances we didn't take.\",\n",
    "    \"She wasn't looking for a knight, she was looking for a sword.\",\n",
    "    \"I dreamt I am running on sand in the night\",\n",
    "]\n",
    "\n",
    "# Paraphrase the sentences\n",
    "for sentence in sentences:\n",
    "    # Tokenize the input sentence\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate paraphrased sentence\n",
    "    paraphrase_ids = model.generate(\n",
    "        input_ids, num_beams=5, max_length=100, early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode and print the paraphrased sentence\n",
    "    paraphrase = tokenizer.decode(paraphrase_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Paraphrase: {paraphrase}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 (Text-To-Text Transfer Transformer)\n",
    "\n",
    "- https://huggingface.co/t5-base\n",
    "- https://huggingface.co/docs/transformers/model_doc/t5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97a598f58e248eea5d68641648279b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881ef2b79f714ad28b1c321a4a561a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3635d68a5f43d589cc8d63f3fc0ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f0f50f946f45bfaf72f1e002d45920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d141f25cfe8141239a50266cbb88a3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: She was a storm, not the kind you run from, but the kind you chase.\n",
      "Paraphrase: She was a storm. She was a storm. She was a storm. She was a storm. She was a storm. She was a storm. She was a storm. She was a storm. She was a storm. She was a storm. She was a storm. She was a storm. She was a storm.....\n",
      "\n",
      "Original: In the end, we only regret the chances we didn't take.\n",
      "Paraphrase: take. We only regret the chances we didn't take.. We only regret the chances we didn't take.... only regret the chances we didn't take....... We only regret the chances we didn't take...... only regret the chances we didn't take. . . ...\n",
      "\n",
      "Original: She wasn't looking for a knight, she was looking for a sword.\n",
      "Paraphrase: a knight, she wasn't looking for a sword.. a sword. a sword.a sword. a sword. She wasn't looking for a knight, she wasn't looking for a sword. a sword...... She was looking for a sword........ a sword.\n",
      "\n",
      "Original: I dreamt I am running on sand in the night\n",
      "Paraphrase: I am running on sand.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 Base model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", model_max_length=1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
    "\n",
    "# Set up input sentences\n",
    "sentences = [\n",
    "    \"She was a storm, not the kind you run from, but the kind you chase.\",\n",
    "    \"In the end, we only regret the chances we didn't take.\",\n",
    "    \"She wasn't looking for a knight, she was looking for a sword.\",\n",
    "    \"I dreamt I am running on sand in the night\",\n",
    "]\n",
    "\n",
    "# Paraphrase the sentences\n",
    "for sentence in sentences:\n",
    "    # Tokenize the input sentence\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate paraphrased sentence\n",
    "    paraphrase_ids = model.generate(\n",
    "        input_ids, num_beams=5, max_length=100, early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode and print the paraphrased sentence\n",
    "    paraphrase = tokenizer.decode(paraphrase_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Paraphrase: {paraphrase}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus Paraphrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: She was a storm, not the kind you run from, but the kind you chase.\n",
      "Paraphrase: She was a storm, not the kind you run from, but the kind you chase.\n",
      "\n",
      "Original: She wasn't looking for a knight, she was looking for a sword.\n",
      "Paraphrase: She was looking for a sword, not a knight.\n",
      "\n",
      "Original: In the end, we only regret the chances we didn't take.\n",
      "Paraphrase: We regret the chances we didn't take.\n",
      "\n",
      "Original: I dreamt I am running on sand in the night\n",
      "Paraphrase: I ran on the sand in the night.\n",
      "\n",
      "Original: Long long ago, there lived a king and a queen. For a long time, they had no children.\n",
      "Paraphrase: They had no children for a long time.\n",
      "\n",
      "Original: I am typing the best article on paraphrasing with Transformers.\n",
      "Paraphrase: I am writing the best article on the subject.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# load pre-trained Pegasus Paraphrase model and tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "# input sentences\n",
    "sentences = [\n",
    "    \"She was a storm, not the kind you run from, but the kind you chase.\",\n",
    "    \"She wasn't looking for a knight, she was looking for a sword.\",\n",
    "    \"In the end, we only regret the chances we didn't take.\",\n",
    "    \"I dreamt I am running on sand in the night\",\n",
    "    \"Long long ago, there lived a king and a queen. For a long time, they had no children.\",\n",
    "    \"I am typing the best article on paraphrasing with Transformers.\",\n",
    "]\n",
    "\n",
    "# Paraphrase the sentences\n",
    "for sentence in sentences:\n",
    "    # Tokenize the input sentence\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate paraphrased sentence\n",
    "    paraphrase_ids = model.generate(\n",
    "        input_ids, num_beams=5, max_length=100, early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode and print the paraphrased sentence\n",
    "    paraphrase = tokenizer.decode(paraphrase_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Paraphrase: {paraphrase}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Sir Henry and I sat at breakfast, the sunlight flooded in through the high windows, causing watery patches of color from the coats of arms. The dark panelling glowed like bronze in the golden rays, and it was hard to see that it was the chamber which had struck such a gloom into our souls the evening before. The evening before, Sir Henry's nerves were still handled and he came to breakfast, his cheeks flushed from the excitement of the early chase.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Load the Pegasus Paraphrase model and tokenizer\n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# function to paraphrase long texts by adjusting the input length\n",
    "def paraphrase_paragraph(text):\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split(\".\")\n",
    "    paraphrases = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Clean up sentences\n",
    "\n",
    "        # remove extra whitespace\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        # filter out empty sentences\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            sentence, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "        )\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        # paraphrase\n",
    "        paraphrase = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=4,\n",
    "            max_length=100,\n",
    "            early_stopping=True,\n",
    "        )[0]\n",
    "        paraphrased_text = tokenizer.decode(paraphrase, skip_special_tokens=True)\n",
    "\n",
    "        paraphrases.append(paraphrased_text)\n",
    "\n",
    "    # Combine the paraphrases\n",
    "    combined_paraphrase = \" \".join(paraphrases)\n",
    "\n",
    "    return combined_paraphrase\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"As Sir Henry and I sat at breakfast, the sunlight flooded in through the high mullioned windows, throwing watery patches of color from the coats of arms which covered them. The dark panelling glowed like bronze in the golden rays, and it was hard to realize that this was indeed the chamber which had struck such a gloom into our souls upon the evening before. But the evening before, Sir Henry's nerves were still handled the stimulant of suspense, and he came to breakfast, his cheeks flushed in the exhilaration of the early chase.\"\n",
    "paraphrase = paraphrase_paragraph(text)\n",
    "print(paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# importing the PEGASUS Transformer model\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "\n",
    "# setting up the model\n",
    "def get_response(input_text, num_return_sequences):\n",
    "    batch = tokenizer.prepare_seq2seq_batch(\n",
    "        [input_text],\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=60,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(torch_device)\n",
    "    translated = model.generate(\n",
    "        **batch,\n",
    "        max_length=60,\n",
    "        num_beams=10,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joana/Documents/Profissional/Projetos/Doing/text-processing-api/text_processing_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4235: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/Users/joana/Documents/Profissional/Projetos/Doing/text-processing-api/text_processing_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I will show you how to use the SweetViz and its dependent library to build a web application.',\n",
       " 'I will show you how to use the SweetViz library to build a web application.',\n",
       " 'I will show you how to build a web application using the SweetViz and its dependent library.',\n",
       " 'I will show you how to use the SweetViz and its dependent library to build a web application in Python.',\n",
       " 'I will show you how to build a web application in Python using the SweetViz library.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test input sentence\n",
    "text = \"I will be showing you how to build a web application in Python using the SweetViz and its dependent library.\"\n",
    "\n",
    "# printing response\n",
    "get_response(text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I will be showing you how to build a web application in Python using the SweetViz and its dependent library.',\n",
       " 'Data science combines multiple fields, including statistics, scientific methods, artificial intelligence (AI), and data analysis, to extract value from data.',\n",
       " 'Those who practice data science are called data scientists, and they combine a range of skills to analyze data collected from the web, smartphones, customers, sensors, and other sources to derive actionable insights.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paragraph of text\n",
    "context = \"I will be showing you how to build a web application in Python using the SweetViz and its dependent library. Data science combines multiple fields, including statistics, scientific methods, artificial intelligence (AI), and data analysis, to extract value from data. Those who practice data science are called data scientists, and they combine a range of skills to analyze data collected from the web, smartphones, customers, sensors, and other sources to derive actionable insights.\"\n",
    "\n",
    "# Takes the input paragraph and splits it into a list of sentences\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "\n",
    "splitter = SentenceSplitter(language=\"en\")\n",
    "\n",
    "sentence_list = splitter.split(context)\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-splitter\n",
      "  Downloading sentence_splitter-1.4-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: regex>=2017.12.12 in /Users/joana/Documents/Profissional/Projetos/Doing/text-processing-api/text_processing_env/lib/python3.11/site-packages (from sentence-splitter) (2024.9.11)\n",
      "Downloading sentence_splitter-1.4-py2.py3-none-any.whl (44 kB)\n",
      "Installing collected packages: sentence-splitter\n",
      "Successfully installed sentence-splitter-1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/joana/Documents/Profissional/Projetos/Doing/text-processing-api/text_processing_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4235: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/Users/joana/Documents/Profissional/Projetos/Doing/text-processing-api/text_processing_env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph before Paraphrased \n",
      "Born in England in 1996, Tom Holland joined the London production of Billy Elliot the Musical in 2008. He soon found success in film, drawing strong reviews for his performance in The Impossible (2012). Tapped to take over the iconic role of Peter Parker/Spider-Man for the big screen, Holland made his debut as the superhero in Captain America: Civil War (2016), before earning the chance to carry his own feature with Spider-Man: Homecoming (2017).\n",
      "\n",
      " ------------------------------------------- \n",
      "\n",
      "Paragraph after Paraphrased \n",
      "In 2008 Tom Holland joined the London production of Billy Elliot the Musical. He got good reviews for his performance in The Impossible. After taking over the role of Spider-Man for the big screen, Holland made his debut as the superhero in Captain America: Civil War, before earning the chance to carry his own feature with Spider-Man: Homecoming.\n"
     ]
    }
   ],
   "source": [
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "model_name = \"tuner007/pegasus_paraphrase\"\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "\n",
    "def get_response(input_text, num_return_sequences):\n",
    "    batch = tokenizer.prepare_seq2seq_batch(\n",
    "        [input_text],\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=60,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(torch_device)\n",
    "    translated = model.generate(\n",
    "        **batch,\n",
    "        max_length=60,\n",
    "        num_beams=10,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "\n",
    "# context = input(\"Enter Paragraph to be Paraphrased: \")\n",
    "# print(context)\n",
    "\n",
    "context = \"Born in England in 1996, Tom Holland joined the London production of Billy Elliot the Musical in 2008. He soon found success in film, drawing strong reviews for his performance in The Impossible (2012). Tapped to take over the iconic role of Peter Parker/Spider-Man for the big screen, Holland made his debut as the superhero in Captain America: Civil War (2016), before earning the chance to carry his own feature with Spider-Man: Homecoming (2017).\"\n",
    "\n",
    "splitter = SentenceSplitter(language=\"en\")\n",
    "\n",
    "sentence_list = splitter.split(context)\n",
    "sentence_list\n",
    "\n",
    "paraphrase = []\n",
    "\n",
    "for i in sentence_list:\n",
    "    a = get_response(i, 1)\n",
    "    paraphrase.append(a)\n",
    "\n",
    "paraphrase2 = [\" \".join(x) for x in paraphrase]\n",
    "paraphrase2\n",
    "\n",
    "paraphrase3 = [\" \".join(x for x in paraphrase2)]\n",
    "paraphrased_text = str(paraphrase3).strip(\"[]\").strip(\"'\")\n",
    "paraphrased_text\n",
    "\n",
    "print(\"Paragraph before Paraphrased \\n\" + context)\n",
    "print(\"\\n ------------------------------------------- \\n\")\n",
    "print(\"Paragraph after Paraphrased \\n\" + paraphrased_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Born in England in 1996, Tom Holland joined the London production of Billy Elliot the Musical in 2008. He soon found success in film, drawing strong reviews for his performance in The Impossible (2012). Tapped to take over the iconic role of Peter Parker/Spider-Man for the big screen, Holland made his debut as the superhero in Captain America: Civil War (2016), before earning the chance to carry his own feature with Spider-Man: Homecoming (2017).\"\n",
    "\n",
    "splitter = SentenceSplitter(language=\"en\")\n",
    "\n",
    "sentence_list = splitter.split(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_processing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
